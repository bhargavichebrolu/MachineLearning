# TOPIC 1 — DATA SOURCES (INDUSTRY-GRADE NOTES)

These notes cover **all segments**, including explanations of:
- Data sources
- Structured / Semi-structured / Unstructured data
- Backend logs
- Clickstream events
- Kafka
- Batch vs Streaming  
along with the doubts discussed.

---

## SEGMENT 1 — WHAT IS A DATA SOURCE (FOUNDATION)

### What a Data Source Is
* A **data source** is the **system that produces data**, not the file itself.
* A CSV, JSON, or database table is a **dataset**, not a data source.

### Key Properties of a Data Source
* Origin — who or what generates the data
* Mechanism — how data is generated
* Frequency — how often data is produced
* Schema behavior — how stable the structure is
* Trust level — how reliable the data is

### Dataset vs Data Source
* **Data Source**: Producer of data (application, sensor, service)
* **Dataset**: Snapshot extracted from a data source

### Raw vs Derived Data
* **Raw data**
  * Directly from the source
  * Messy
  * Minimal processing
* **Derived data**
  * Aggregated or transformed
  * Built from raw data
  * Used for modeling

### First-party vs Third-party Data
* **First-party**
  * Collected by your own system
  * High trust
* **Third-party**
  * Collected by others (e.g., Kaggle, public datasets)
  * Lower trust
  * Must acknowledge limitations

---

## SEGMENT 2 — STRUCTURED DATA (DEEP DIVE)

### What Structured Data Means
* Data with a **fixed schema**
* Schema is **enforced at write time**
* Columns, data types, and constraints are predefined

### Common Structured Data Sources
* Relational databases (MySQL, PostgreSQL)
* CSV / TSV files
* Data warehouses

### Schema Components
* Column names
* Data types
* Nullability
* Relationships (primary keys, foreign keys)

### Common Problems in Structured Data
* Missing values
* Duplicate records
* Wrong data types
* Schema evolution (new / removed columns)

### CSV vs TSV
* **CSV** — Comma-separated values
  * Common
  * Breaks easily when text contains commas
* **TSV** — Tab-separated values
  * Safer for text-heavy data
  * Less parsing ambiguity

---

## SEGMENT 3 — SEMI-STRUCTURED DATA (JSON, LOGS)

### What Semi-Structured Data Is
* Data **has structure**
* Structure is **not enforced**
* Schema is applied at read time (schema-on-read)

### Common Sources
* Application logs
* Clickstream events
* API responses
* Event systems

### JSON Characteristics
* Nested objects
* Optional keys
* Arrays
* Inconsistent structure across records

### Common Problems
* Missing keys
* Variable nesting
* Corrupted records
* Late detection of schema issues

### Schema-on-Read
* No validation when data is written
* Errors appear later in pipelines
* High risk for ML systems

---

## BACKEND LOGS (DETAILED EXPLANATION)

### What Backend Logs Are
* Records written by backend services to describe **system behavior**
* Used by developers, testers, SREs, and monitoring systems

### What Backend Logs Are NOT
* Not random print statements
* Not meant for end users

### What Backend Logs Contain
* Timestamp
* Log level (INFO, ERROR, WARN)
* Service name
* Event or message
* Metadata (user_id, error_code, latency)

### Purpose of Backend Logs
* Debugging
* Monitoring
* Auditing
* Security analysis
* ML and anomaly detection

### Key Insight
* Logs show **what happened**, not magically why something didn’t happen
* Engineers infer causes from log evidence

---

## CLICKSTREAM EVENTS

### What Clickstream Events Are
* Records of **user interactions** with a system
* Generated by frontend applications

### Examples
* Page views
* Button clicks
* Add to cart
* Purchases

### Properties
* High volume
* Semi-structured (usually JSON)
* Fields vary by event type

### ML Use Cases
* Recommendation systems
* Churn prediction
* User behavior modeling
* Funnel analysis

### Important Distinction
* Backend logs → system behavior
* Clickstream events → user behavior

---

## KAFKA (FROM ZERO)

### What Kafka Is
* Kafka is **not data**
* Kafka is **not a database**
* Kafka is a **transport system for events**

### Simple Definition
* Kafka collects events and delivers them to whoever needs them, reliably and in order.

### Kafka Mental Model
* Kafka is like a **post office** or **conveyor belt**
* Producers send events to Kafka
* Consumers read events from Kafka

### What an Event Is
* A message describing something that happened
* Example:
  * User clicked
  * Payment failed
  * Sensor reading

### Producers
* Backend services
* Frontend trackers
* Sensors
* APIs

### Consumers
* ML pipelines
* Databases
* Monitoring systems
* Alerting systems

### What Kafka Does
* Stores events temporarily
* Preserves order
* Scales to very high volume

### What Kafka Does NOT Do
* No data cleaning
* No validation
* No understanding of meaning
* No analytics

---

## SEGMENT 5 — BATCH vs STREAMING (TIME & VELOCITY)

### Core Idea
* Difference is **WHEN data is processed**
* Not about tools or file formats

### Batch Processing
* Data is collected
* System waits
* Processes data later in bulk

**Characteristics**
* High latency
* Easier debugging
* Easier reprocessing

**Examples**
* End-of-day transaction CSV
* Nightly model training
* Weekly reports

### Streaming Processing
* Data is processed immediately as it arrives

**Characteristics**
* Low latency
* Harder debugging
* Ordering and duplication issues

**Examples**
* Fraud detection during payment
* Live recommendations
* Real-time alerts

### Key Rule
* Batch = wait first, then process
* Streaming = process immediately

### Kafka and Batch/Streaming
* Kafka does not decide batch or streaming
* Kafka can support both
  * Batch: read events later
  * Streaming: read events immediately

---

## ONE-LINE INTERVIEW TAKEAWAYS

* A data source is the system that produces data, not the file.
* Structured data enforces schema at write time.
* Semi-structured data has schema-on-read.
* Backend logs describe system behavior.
* Clickstream events describe user behavior.
* Kafka is an event transport system.
* Batch systems prioritize correctness.
* Streaming systems prioritize timeliness.
